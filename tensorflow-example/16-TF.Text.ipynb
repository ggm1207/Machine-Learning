{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTF-8로 작업을 해야 좋다.\n",
    "\n",
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "# 유니 코드 스크립트 경계를 기반으로 UTF-8 문자열을 분할합니다.\n",
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([25, 25, 25,  0], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_script([ord(c) for c in u'Sad☹'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "# 공백없는 언어를 토큰화하여 단어를 분할 할 때는 이렇게 하는 것이 좋다.\n",
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 토큰의 위치를 알고 싶을 때\n",
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list()) # 시작 위치\n",
    "print(offset_limits.to_list()) # 끝 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n",
      "[[False, False, False, False, False, True], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n"
     ]
    }
   ],
   "source": [
    "# Wordshape는 입력 텍스트에서 다양한 관련 패턴을 일치시키기 위해\n",
    "# 유용한 정규식 기반 도우미 함수를 다양하게 정의합니다.\n",
    "\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'Everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]>\n",
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens)\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
